---
title: "Chapter 5 - Exercises"
author: "Corrie"
date: "June 3, 2018"
output: html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Chapter 5 - Exercises
These are my solutions to the exercises from chapter 5.

## Easy.
__5E1.__ The following linear models are multiple linear regressions:

- $\mu_i = \beta_x x_i + \beta_z z_i $
- $\mu_i = \alpha + \beta_x x_i + \beta_z z_i$
whereas the following are bivariate linear regressions:

- $\mu_i = \alpha + \beta x_i$
- $\mu_i = \alpha + \beta(x_i - z_i) $

__5E2.__ Write down a multiple regression to evaluate the claim: _Animal diversity is linearly related to latitude, but only after controlling for plant diversity._
$$
\begin{align*}
\text{animal diversity}_i &\sim \text{Normal}( \mu_i, \sigma) \\
\mu_i &= \alpha + \beta_{lat}\text{latitude}_i + \beta_{plant}\text{plant diversity}_i 
\end{align*}$$
__5E3.__ Write down a multiple regression to evaluate the claim:
_Neither amount of funding nor size of laboratory is by itself a good predictor of time to PhD degree; but together these variables are both positively associated with time to degree._

$$\begin{align*}
\text{time to PhD}_i &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_{f}\text{funding}_i + \beta_{l} \text{lab size}_i
\end{align*}$$
The parameters $\beta_f$ and $\beta_l$ should be both positive ($>0$).

__5E4.__ Categorical predictor with 4 levels, labeled A, B, C, and D. $A_i$, $B_i$, $C_i$, and $D_i$ are the respective indicator variables.
The following are inferentially equivalent ways to include the categorical variables in a regression:

- $\mu_i = \alpha + \beta_A A_i + \beta_B B_i + \beta_C C_i$
- $\mu_i = \alpha + \beta_B B_i + \beta_C C_i + \beta_D D_i$

## Medium.
__5M1.__ Invent your own examples of a spurious correlations. An outcome variable should be correlated with both predictor variables. But when both predictors are entered in the same model, the correlation wbetween the outcome and one of the predictors should mostly vanish.
```{r}
n <- 100
x_real <- rnorm(n)
x_spur <- rnorm(n, x_real)
y <- rnorm(n, x_real)

df <- data.frame(x_real=x_real, x_spur=x_spur, y=y)
pairs(df)
```
```{r, message=FALSE}
library(rethinking)
mod1 <- lm(y ~ ., data=df)
plot( precis(mod1) )
```
Note: I found an article about an interesting spurious correlation: There are various correlations between a name of a person and where they live, what they work, and whom they marry. Apparently, people with similar names tend to marry each other and similarly, choose occupations that sound similar to their own name (e.g. Dennis - dentist). These are spurious correlations: people of the same age tend to marry each other and people of the same age tend to have similar names. The [article](http://andrewgelman.com/2011/02/09/dennis_the_dent/) gives a short summary of the confounding variables 

__5M2.__ Invent your own example of a masked relationship.
```{r}
n <- 100
rho <- 0.7                           # correlation between x_pos and x_neg
x_pos <- rnorm( n )                  # x_pos as Gaussian
x_neg <- rnorm( n, rho*x_pos,        # x_neg correlated with x_pos
                sqrt(1-rho^2) )
y <- rnorm( n, x_pos - x_neg)        # y equally associated with x_pos, x_eg
df <- data.frame(x_pos = x_pos, x_neg=x_neg, y=y)
pairs(df)
```
The variables `x_pos` and `x_neg` are correlated with each other but based on the plot, it looks as if they don't associate much with the outcome `y`.
```{r}
mod2 <- lm(y ~ ., data=df)
plot( precis(mod2))
```
The unmasks the association and shows that both variables have an association with `y`.

__5M3.__
It is sometimes observed that the best predictor of fire risk is the presence of firefighters: States and localities with many firefighters also have more fires. Now, firefighters do not _cause_ fires, but this is not a spurious correlation. Instead, it is fire tat causes firefighters.
In the context of divorce and marriage data: How might a high divorce rate cause a higher marriage rate?
A high divorce rate means that there is also a higher number of people that can marry again, thus raising the marriage rate. One way I can think of testing this relationship would be to include the remarriage rate in a multivariate regression.

__5M4.__ In the divorce data, States with high number of Mormons have much lower divorce rates than the regression model expected. Include percent of Latter-day Saints, LDS, in your regression model.
I first downloaded the LDS population by State from [wikipedia](https://en.wikipedia.org/wiki/The_Church_of_Jesus_Christ_of_Latter-day_Saints_membership_statistics_(United_States)) (retrieved on June 6, 2018). Next step is to combine the two data frames:
```{r, message=FALSE, warning=FALSE}
library(dplyr)
data("WaffleDivorce")
d <- WaffleDivorce
LDS <- read.csv("LDS.csv")
d <- d %>% left_join(LDS, by=c("Location" = "State")) %>% 
  select(Location, Loc, MedianAgeMarriage, Marriage, Divorce, LDS) %>%
  mutate(Location = as.factor(Location))
head(d)
```
```{r}
hist(d$LDS)
```
Since the LDS variable is very skewed (most states have almost no LDS population, a few, e.g. Idaho and Utah, have a very high LDS population), so it would be better to transform it. We use first a log-transform and then standardize the variable.
```{r}
d$log.LDS <- log(d$LDS)
d$log.LDS.s <- ( d$log.LDS - mean(d$log.LDS) ) / sd(d$log.LDS)
d$LDS.s <- (d$LDS - mean(d$LDS)) / sd(d$LDS)
hist(d$log.LDS.s)
```
We also need to standardize the other variables:
```{r}
d$MedianAgeMarriage.s <- (d$MedianAgeMarriage - mean(d$MedianAgeMarriage)) / sd(d$MedianAgeMarriage)
d$Marriage.s <- (d$Marriage - mean(d$Marriage)) / sd(d$Marriage)
```
Build the model:
```{r}
mod4 <- map(
  alist(
    Divorce ~ dnorm( mu, sigma),
    mu <- a + bMA*MedianAgeMarriage.s + bMR*Marriage.s + bL*log.LDS.s,
    a ~ dnorm( 0, 10 ),
    c(bMA, bMR, bL) ~ dnorm( 0, 1),
    sigma ~ dunif(0, 10)
  ), data=d
)
precis( mod4 )
```
```{r}
plot(precis( mod4))
```
This means, that a higher population is negatively associated with the divorce rate.

```{r, fig1, fig.width=8, fig.height=8}
mu <- link(mod4)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

divorce.sim <- sim( mod4, n=1e4 )
divorce.PI <- apply(divorce.sim, 2, PI)
# residual plot showing the mean prediction error
# compute residuals
divorce.resid <- d$Divorce - mu.mean
# get ordering by divorce rate
o <- order(divorce.resid)
# make the plot
dotchart( divorce.resid[o], labels=d$Loc[o], xlim=c(-6,5), cex=0.6 )
abline(v=0, col=col.alpha("black", 0.2))
for (i in 1:nrow(d) ) {
  j <- o[i]    # which State in order
  lines( d$Divorce[j] - c(mu.PI[1, j], mu.PI[2, j]), rep(i,2) )
  points( d$Divorce[j] - c(divorce.PI[1,j], divorce.PI[2, j]), rep(i,2) ,
          pch=3, cex=0.6, col="gray")
}
```
The model still overestimates the divorce rate for Idaho, but whereas before (without the LDS predictor) it had a mean prediction error of about  -4.4, it now has a mean prediction error of about
```{r}
divorce.resid[d$Loc == "ID"]
```
The mean prediction error for Utah improved similarly.

__5M5.__ One way to reasoin through multiple causation hypotheses is to imagine detailed mechanisms through which predictor variables might influence outcomes. 
Example: It is sometimes argued that the price of gasoline (predictor variable) is positively associated with lower obesity rates (outcome variable).
There are two important mechanisms by which the price of as could reduce obesity.
(1) high gas prices lead to less driving and thus more walking
(2) high gas prices lead to less driving, which leads to less eating out.
What multiple regression variables could we use to address these mechanisms? (assuming we can have any predictor variable we want)
For the first case, we could include the predictor variable of the average walked distance. To address the second case, a good variable to include would be the average rate of eating out.

## Hard.
All three exercises below use the data `foxes` about the urban fox _Vulpes vulpes_. They move in packs and defend territories, so data on habitat quality and population density is included in the data. It also includes the number of social groups an individual fox belongs to, weight of an individual fox, as well as the average amount of food available in a territory, the group size and area of the territory.
```{r}
data(foxes)
d <- foxes
head(d)
```
```{r}
summary(d)
```

__5H1.__ Fit two bivariate Gaussian regressions, using `map`:
  (1) body weight ~ territory size (area)
  (2) body weight ~ groupsize
```{r}
d$area.s <- (d$area - mean(d$area)) / sd(d$area)
mod5 <- map(
  alist(
    weight ~ dnorm( mu, sigma) ,
    mu <- a + bA*area.s ,
    a ~ dnorm( 0, 100),
    bA ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)
  ), data=d
)
plot( precis( mod5))
```

It looks like area is not an important predictor for body weight. Let's have a closer look at some more plots.
```{r}
area.seq <- seq(from=-2.5, to=2.5, length.out = 300)
mu <- link(mod5, data=list(area.s = area.seq))
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob=0.95)
weight.sim <- sim(mod5, data=list(area.s = area.seq))
weight.HPDI <- apply( weight.sim, 2, HPDI, prob=0.95)
plot(  weight ~ area.s, data=d)
lines( area.seq, mu.mean)
shade( mu.HPDI, area.seq)
shade( weight.HPDI, area.seq)
```
This plot also suggests that area is not an important predictor. There seems to be no relation at all with area and sigma is relatively large.
```{r}
d$groupsize.s <- (d$groupsize - mean(d$groupsize)) / sd(d$groupsize)
mod6 <- map(
  alist(
    weight ~ dnorm( mu, sigma) ,
    mu <- a + bG*groupsize.s,
    bG ~ dnorm(0, 1),
    a ~ dnorm(0, 10),
    sigma ~ dunif(0, 10)
  ), data=d
)
plot( precis( mod6))
```

The groupsize seems to have some importance, at least it is slightly further away from 0 than the parameter for area.
```{r}
groupsize.seq <- seq(from=-2, to=3, length.out = 300)
mu <- link( mod6, data=list(groupsize.s=groupsize.seq))
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob=0.95)
weight.sim <- sim(mod6, data=list(groupsize.s=groupsize.seq))
weight.HPDI <- apply(weight.sim, 2, HPDI, prob=0.95)
plot( weight ~ groupsize.s, data=d)
lines(groupsize.seq, mu.mean)
shade(mu.HPDI, groupsize.seq)
shade(weight.HPDI, groupsize.seq)
```

While there seems to be more of a slope here, it is still very minor and doesn't look like it is an important factor..

__5H2.__ As before, we try to predict weight, but this time using a multivariate model that uses both the area and groupsize as predictor.
```{r}
mod7 <- map(
  alist(
    weight ~ dnorm(mu, sigma),
    mu <- a + bA*area.s + bG*groupsize.s,
    a ~ dnorm(0, 10),
    bA ~ dnorm(0,1),
    bG ~ dnorm(0,1),
    sigma ~ dunif(0,10)
  ), data=d
)
plot( precis( mod7))
```

And surprise surprise, suddenly both area and groupsize seem to have a discernible importance for predicting body weight. Let's plot the predictions of the model for both predictors, for each holding the other predictor constant at its mean.
```{r}
# Area, holding groupsize fixed
groupsize.avg <- mean(d$groupsize.s)
area.seq <- seq(from=-3, to=3, length.out = 300)
pred.data <- data.frame(groupsize.s=groupsize.avg, area.s=area.seq)
mu <- link(mod7, data=pred.data)
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob=0.95)
weight.sim <- sim(mod7, data=pred.data)
weight.HPDI <- apply(weight.sim, 2, HPDI, prob=0.95)
plot( weight ~ area.s, data=d, type="n" )
lines(area.seq, mu.mean)
shade(mu.HPDI, area.seq)
shade(weight.HPDI, area.seq)
```

Now, we can see a quite clear association between area and weight: A fox living in a bigger territory seems to be heavier than a fox in a smaller territory.
```{r}
# Groupsize, area fixed
area.avg <- mean(d$area.s)
groupsize.seq <- seq(from=-2, to=3, length.out = 300)
pred.data <- data.frame(groupsize.s=groupsize.seq, area.s=area.avg)
mu <- link(mod7, data=pred.data)
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob=0.95)
weight.sim <- sim(mod7, data=pred.data)
weight.HPDI <- apply(weight.sim, 2, HPDI, prob=0.95)
plot( weight ~ groupsize.s, data=d, type="n")
lines( groupsize.seq, mu.mean)
shade( mu.HPDI, groupsize.seq)
shade( weight.HPDI, groupsize.seq)
```

A larger group is associated with a lower weight (more foxes that need to share food, maybe?).

So why is this? Let's look at the correlation plot of the three variables.
```{r}
pairs( weight ~ groupsize + area, data=d)
```

Groupsize is strongly correlated with area: a larger territory is associated with a larger group of foxes (makes sense, right?). Both area and groupsize are also correlated with weight: area is positively correlated with weight, while groupsize is negatively correlated with weight. This circumstance leads to a masked association: the two predictor variables cancel each other out and thus don't seem important.




## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
